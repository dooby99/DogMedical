{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms,datasets\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# dir_data = os.getcwd()\n",
        "dir_data = \"D:\\\\runnable\"\n",
        "dir_data\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        def CBR2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):\n",
        "            layers = []\n",
        "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                                 bias=bias)]\n",
        "            layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
        "            layers += [nn.ReLU()]\n",
        "\n",
        "            cbr = nn.Sequential(*layers)\n",
        "\n",
        "            return cbr\n",
        "\n",
        "        # Contracting path\n",
        "        self.enc1_1 = CBR2d(in_channels=3, out_channels=64)\n",
        "        self.enc1_2 = CBR2d(in_channels=64, out_channels=64)\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc2_1 = CBR2d(in_channels=64, out_channels=128)\n",
        "        self.enc2_2 = CBR2d(in_channels=128, out_channels=128)\n",
        "\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc3_1 = CBR2d(in_channels=128, out_channels=256)\n",
        "        self.enc3_2 = CBR2d(in_channels=256, out_channels=256)\n",
        "\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc4_1 = CBR2d(in_channels=256, out_channels=512)\n",
        "        self.enc4_2 = CBR2d(in_channels=512, out_channels=512)\n",
        "\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc5_1 = CBR2d(in_channels=512, out_channels=1024)\n",
        "\n",
        "        # Expansive path\n",
        "        self.dec5_1 = CBR2d(in_channels=1024, out_channels=512)\n",
        "\n",
        "        self.unpool4 = nn.ConvTranspose2d(in_channels=512, out_channels=512,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec4_2 = CBR2d(in_channels=2 * 512, out_channels=512)\n",
        "        self.dec4_1 = CBR2d(in_channels=512, out_channels=256)\n",
        "\n",
        "        self.unpool3 = nn.ConvTranspose2d(in_channels=256, out_channels=256,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec3_2 = CBR2d(in_channels=2 * 256, out_channels=256)\n",
        "        self.dec3_1 = CBR2d(in_channels=256, out_channels=128)\n",
        "\n",
        "        self.unpool2 = nn.ConvTranspose2d(in_channels=128, out_channels=128,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec2_2 = CBR2d(in_channels=2 * 128, out_channels=128)\n",
        "        self.dec2_1 = CBR2d(in_channels=128, out_channels=64)\n",
        "\n",
        "        self.unpool1 = nn.ConvTranspose2d(in_channels=64, out_channels=64,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec1_2 = CBR2d(in_channels=2 * 64, out_channels=64)\n",
        "        self.dec1_1 = CBR2d(in_channels=64, out_channels=64)\n",
        "\n",
        "        self.fc = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1_1 = self.enc1_1(x)\n",
        "        enc1_2 = self.enc1_2(enc1_1)\n",
        "        pool1 = self.pool1(enc1_2)\n",
        "\n",
        "        enc2_1 = self.enc2_1(pool1)\n",
        "        enc2_2 = self.enc2_2(enc2_1)\n",
        "        pool2 = self.pool2(enc2_2)\n",
        "\n",
        "        enc3_1 = self.enc3_1(pool2)\n",
        "        enc3_2 = self.enc3_2(enc3_1)\n",
        "        pool3 = self.pool3(enc3_2)\n",
        "\n",
        "        enc4_1 = self.enc4_1(pool3)\n",
        "        enc4_2 = self.enc4_2(enc4_1)\n",
        "        pool4 = self.pool4(enc4_2)\n",
        "\n",
        "        enc5_1 = self.enc5_1(pool4)\n",
        "\n",
        "        dec5_1 = self.dec5_1(enc5_1)\n",
        "\n",
        "        unpool4 = self.unpool4(dec5_1)\n",
        "        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n",
        "        dec4_2 = self.dec4_2(cat4)\n",
        "        dec4_1 = self.dec4_1(dec4_2)\n",
        "\n",
        "        unpool3 = self.unpool3(dec4_1)\n",
        "        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n",
        "        dec3_2 = self.dec3_2(cat3)\n",
        "        dec3_1 = self.dec3_1(dec3_2)\n",
        "\n",
        "        unpool2 = self.unpool2(dec3_1)\n",
        "        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n",
        "        dec2_2 = self.dec2_2(cat2)\n",
        "        dec2_1 = self.dec2_1(dec2_2)\n",
        "\n",
        "        unpool1 = self.unpool1(dec2_1)\n",
        "        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n",
        "        dec1_2 = self.dec1_2(cat1)\n",
        "        dec1_1 = self.dec1_1(dec1_2)\n",
        "\n",
        "        x = self.fc(dec1_1)\n",
        "\n",
        "        return x\n",
        "    \n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        lst_data = os.listdir(self.data_dir)\n",
        "\n",
        "        lst_label = [f for f in lst_data if f.startswith('label')]  # \ubb38\uc790\uc5f4 \uac80\uc0ac\ud574\uc11c 'label'\uc774 \uc788\uc73c\uba74 True\n",
        "        lst_input = [f for f in lst_data if f.startswith('input')]  # \ubb38\uc790\uc5f4 \uac80\uc0ac\ud574\uc11c 'input'\uc774 \uc788\uc73c\uba74 True\n",
        "\n",
        "        lst_label.sort()\n",
        "        lst_input.sort()\n",
        "\n",
        "        self.lst_label = lst_label\n",
        "        self.lst_input = lst_input\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lst_label)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        try:\n",
        "            label = np.load(os.path.join(self.data_dir, self.lst_label[index]))\n",
        "            inputs = np.load(os.path.join(self.data_dir, self.lst_input[index]))\n",
        "\n",
        "            # normalize\n",
        "            label = label / 255.0\n",
        "            inputs = inputs / 255.0\n",
        "            label = label.astype(np.float32)\n",
        "            inputs = inputs.astype(np.float32)\n",
        "\n",
        "            # \uc778\ud48b \ub370\uc774\ud130 \ucc28\uc6d0\uc774 2\uc774\uba74, \ucc44\ub110 \ucd95\uc744 \ucd94\uac00\ud574\uc918\uc57c\ud55c\ub2e4. \ud30c\uc774\ud1a0\uce58 \uc778\ud48b\uc740 (batch, \ucc44\ub110, \ud589, \uc5f4)\n",
        "            if label.ndim == 2:\n",
        "                label = label[:, :, np.newaxis]\n",
        "            if inputs.ndim == 2:\n",
        "                inputs = inputs[:, :, np.newaxis]\n",
        "\n",
        "            data = {'input': inputs, 'label': label}\n",
        "\n",
        "            if self.transform:\n",
        "                data = self.transform(data)  # transform\uc5d0 \ud560\ub2f9\ub41c class \ub4e4\uc774 \ud638\ucd9c\ub418\uba74\uc11c __call__ \ud568\uc218 \uc2e4\ud589\n",
        "        except:\n",
        "            print(\"\uc624\ub958 \ubc1c\uc0dd -> \")\n",
        "            print(os.path.join(self.data_dir, self.lst_label[index]))\n",
        "            print(os.path.join(self.data_dir, self.lst_input[index]))\n",
        "\n",
        "        return data \n",
        "    \n",
        "class ToTensor(object):\n",
        "    def __call__(self, data):\n",
        "        label, input = data['label'], data['input']\n",
        "\n",
        "        label = label.transpose((2, 0, 1)).astype(np.float32)\n",
        "        input = input.transpose((2, 0, 1)).astype(np.float32)\n",
        "\n",
        "        data = {'label': torch.from_numpy(label), 'input': torch.from_numpy(input)}\n",
        "\n",
        "        return data\n",
        "    \n",
        "class Normalization(object):\n",
        "    def __init__(self, mean=0.5, std=0.5):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, data):\n",
        "        label, input = data['label'], data['input']\n",
        "\n",
        "        input = (input - self.mean) / self.std\n",
        "\n",
        "        data = {'label': label, 'input': input}\n",
        "\n",
        "        return data\n",
        "    \n",
        "class RandomFlip(object):\n",
        "    def __call__(self, data):\n",
        "        label, input = data['label'], data['input']\n",
        "\n",
        "        if np.random.rand() > 0.5:\n",
        "            label = np.fliplr(label)\n",
        "            input = np.fliplr(input)\n",
        "\n",
        "        if np.random.rand() > 0.5:\n",
        "            label = np.flipud(label)\n",
        "            input = np.flipud(input)\n",
        "\n",
        "        data = {'label': label, 'input': input}\n",
        "\n",
        "        return data\n",
        "    \n",
        "## \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 \uc124\uc815\n",
        "\n",
        "lr = 1e-3\n",
        "batch_size = 4\n",
        "num_epoch = 100\n",
        "\n",
        "data_dir = os.path.join(dir_data, 'data')\n",
        "ckpt_dir = os.path.join(dir_data, 'checkpoint')\n",
        "log_dir = os.path.join(dir_data, 'log')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# transform \uc801\uc6a9\ud574\uc11c \ub370\uc774\ud130 \uc14b \ubd88\ub7ec\uc624\uae30\n",
        "transform = transforms.Compose([Normalization(mean=0.5, std=0.5), RandomFlip(), ToTensor()])\n",
        "dataset_train = Dataset(data_dir=os.path.join(data_dir,'train'),transform=transform)\n",
        "\n",
        "# \ubd88\ub7ec\uc628 \ub370\uc774\ud130\uc14b, \ubc30\uce58 size\uc918\uc11c DataLoader \ud574\uc8fc\uae30\n",
        "loader_train = DataLoader(dataset_train, batch_size = batch_size, shuffle=True)\n",
        "\n",
        "# val set\ub3c4 \ub3d9\uc77c\ud558\uac8c \uc9c4\ud589\n",
        "dataset_val = Dataset(data_dir=os.path.join(data_dir,'val'),transform = transform)\n",
        "loader_val = DataLoader(dataset_val, batch_size=batch_size , shuffle=True)\n",
        "\n",
        "# \ub124\ud2b8\uc6cc\ud06c \ubd88\ub7ec\uc624\uae30\n",
        "net = UNet().to(device) # device : cpu or gpu\n",
        "\n",
        "# loss \uc815\uc758\n",
        "fn_loss = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "# Optimizer \uc815\uc758\n",
        "optim = torch.optim.Adam(net.parameters(), lr = lr)\n",
        "\n",
        "# \uae30\ud0c0 variables \uc124\uc815\n",
        "num_train = len(dataset_train)\n",
        "num_val = len(dataset_val)\n",
        "\n",
        "num_train_for_epoch = np.ceil(num_train/batch_size) # np.ceil : \uc18c\uc218\uc810 \ubc18\uc62c\ub9bc\n",
        "num_val_for_epoch = np.ceil(num_val/batch_size)\n",
        "\n",
        "# \uae30\ud0c0 function \uc124\uc815\n",
        "fn_tonumpy = lambda x : x.to('cpu').detach().numpy().transpose(0,2,3,1) # device \uc704\uc5d0 \uc62c\ub77c\uac04 \ud150\uc11c\ub97c detach \ud55c \ub4a4 numpy\ub85c \ubcc0\ud658\n",
        "fn_denorm = lambda x, mean, std : (x * std) + mean \n",
        "fn_classifier = lambda x :  1.0 * (x > 0.5)  # threshold 0.5 \uae30\uc900\uc73c\ub85c indicator function\uc73c\ub85c classifier \uad6c\ud604\n",
        "\n",
        "# Tensorbord\n",
        "writer_train = SummaryWriter(log_dir=os.path.join(log_dir,'train'))\n",
        "writer_val = SummaryWriter(log_dir = os.path.join(log_dir,'val'))\n",
        "\n",
        "# \ub124\ud2b8\uc6cc\ud06c \uc800\uc7a5\ud558\uae30\n",
        "# train\uc744 \ub9c8\uce5c \ub124\ud2b8\uc6cc\ud06c \uc800\uc7a5 \n",
        "# net : \ub124\ud2b8\uc6cc\ud06c \ud30c\ub77c\ubbf8\ud130, optim  \ub450\uac1c\ub97c dict \ud615\ud0dc\ub85c \uc800\uc7a5\n",
        "def save(ckpt_dir,net,optim,epoch):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        os.makedirs(ckpt_dir)\n",
        "\n",
        "    torch.save({'net':net.state_dict(),'optim':optim.state_dict()},'%s/model_epoch%d.pth'%(ckpt_dir,epoch))\n",
        "\n",
        "# \ub124\ud2b8\uc6cc\ud06c \ubd88\ub7ec\uc624\uae30\n",
        "def load(ckpt_dir,net,optim):\n",
        "    if not os.path.exists(ckpt_dir): # \uc800\uc7a5\ub41c \ub124\ud2b8\uc6cc\ud06c\uac00 \uc5c6\ub2e4\uba74 \uc778\ud48b\uc744 \uadf8\ub300\ub85c \ubc18\ud658\n",
        "        epoch = 0\n",
        "        return net, optim, epoch\n",
        "    \n",
        "    ckpt_lst = os.listdir(ckpt_dir) # ckpt_dir \uc544\ub798 \uc788\ub294 \ubaa8\ub4e0 \ud30c\uc77c \ub9ac\uc2a4\ud2b8\ub97c \ubc1b\uc544\uc628\ub2e4\n",
        "    ckpt_lst.sort(key = lambda f : int(''.join(filter(str.isdigit,f))))\n",
        "\n",
        "    dict_model = torch.load('%s/%s' % (ckpt_dir,ckpt_lst[-1]))\n",
        "\n",
        "    net.load_state_dict(dict_model['net'])\n",
        "    optim.load_state_dict(dict_model['optim'])\n",
        "    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n",
        "\n",
        "    return net,optim,epoch\n",
        "\n",
        "# \ub124\ud2b8\uc6cc\ud06c \ud559\uc2b5\uc2dc\ud0a4\uae30\n",
        "start_epoch = 0\n",
        "net, optim, start_epoch = load(ckpt_dir = ckpt_dir, net = net, optim = optim) # \uc800\uc7a5\ub41c \ub124\ud2b8\uc6cc\ud06c \ubd88\ub7ec\uc624\uae30\n",
        "\n",
        "for epoch in range(start_epoch+1,num_epoch +1):\n",
        "    net.train()\n",
        "    loss_arr = []\n",
        "\n",
        "    for batch, data in enumerate(loader_train, 1):\n",
        "        # forward\n",
        "        label = data['label'].to(device)   # \ub370\uc774\ud130 device\ub85c \uc62c\ub9ac\uae30     \n",
        "        inputs = data['input'].to(device)\n",
        "        output = net(inputs) \n",
        "\n",
        "        # backward\n",
        "        optim.zero_grad()  # gradient \ucd08\uae30\ud654\n",
        "        loss = fn_loss(output, label)  # output\uacfc label \uc0ac\uc774\uc758 loss \uacc4\uc0b0\n",
        "        loss.backward() # gradient backpropagation\n",
        "        optim.step() # backpropa \ub41c gradient\ub97c \uc774\uc6a9\ud574\uc11c \uac01 layer\uc758 parameters update\n",
        "\n",
        "        # save loss\n",
        "        loss_arr += [loss.item()]\n",
        "\n",
        "        # tensorbord\uc5d0 \uacb0\uacfc\uac12\ub4e4 \uc800\uc815\ud558\uae30\n",
        "        label = fn_tonumpy(label)\n",
        "        inputs = fn_tonumpy(fn_denorm(inputs,0.5,0.5))\n",
        "        output = fn_tonumpy(fn_classifier(output))\n",
        "\n",
        "        writer_train.add_image('label', label, num_train_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
        "        writer_train.add_image('input', inputs, num_train_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
        "        writer_train.add_image('output', output, num_train_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
        "\n",
        "    writer_train.add_scalar('loss', np.mean(loss_arr), epoch)\n",
        "\n",
        "    \n",
        "    # validation\n",
        "    with torch.no_grad(): # validation \uc774\uae30 \ub54c\ubb38\uc5d0 backpropa \uc9c4\ud589 x, \ud559\uc2b5\ub41c \ub124\ud2b8\uc6cc\ud06c\uac00 \uc815\ub2f5\uacfc \uc5bc\ub9c8\ub098 \uac00\uae4c\uc6b4\uc9c0 loss\ub9cc \uacc4\uc0b0\n",
        "        net.eval() # \ub124\ud2b8\uc6cc\ud06c\ub97c evaluation \uc6a9\uc73c\ub85c \uc120\uc5b8\n",
        "        loss_arr = []\n",
        "\n",
        "        for batch, data in enumerate(loader_val,1):\n",
        "            # forward\n",
        "            label = data['label'].to(device)\n",
        "            inputs = data['input'].to(device)\n",
        "            output = net(inputs)\n",
        "\n",
        "            # loss \n",
        "            loss = fn_loss(output,label)\n",
        "            loss_arr += [loss.item()]\n",
        "            print('valid : epoch %04d / %04d | Batch %06d \\ %06d | Loss %.6f'%(epoch,num_epoch,batch,num_val_for_epoch,np.mean(loss_arr)))\n",
        "\n",
        "            # Tensorboard \uc800\uc7a5\ud558\uae30\n",
        "            label = fn_tonumpy(label)\n",
        "            inputs = fn_tonumpy(fn_denorm(inputs, mean=0.5, std=0.5))\n",
        "            output = fn_tonumpy(fn_classifier(output))\n",
        "\n",
        "            writer_val.add_image('label', label, num_val_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
        "            writer_val.add_image('input', inputs, num_val_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
        "            writer_val.add_image('output', output, num_val_for_epoch * (epoch - 1) + batch, dataformats='NHWC')\n",
        "\n",
        "        writer_val.add_scalar('loss', np.mean(loss_arr), epoch)\n",
        "\n",
        "        # epoch\uc774 \ub05d\ub0a0\ub54c \ub9c8\ub2e4 \ub124\ud2b8\uc6cc\ud06c \uc800\uc7a5\n",
        "        if epoch%5 == 0:\n",
        "            save(ckpt_dir=ckpt_dir, net = net, optim = optim, epoch = epoch)\n",
        "\n",
        "writer_train.close()\n",
        "writer_val.close()"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}